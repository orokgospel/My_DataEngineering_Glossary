{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "After completing this lab you will be able to:\n\n- Start Apache Airflow.\n- Open the Airflow UI in a browser.\n- List all the DAGs.\n- List the tasks in a DAG.\n- Explore a DAG in the UI",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Exercise 1** - Start Apache Airflow\nOpen a new terminal by clicking on the menu bar and selecting Terminal->New Terminal",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Run the commands below on the newly opened terminal. (You can copy the code by clicking on the little copy button on the bottom right of the codeblock below and paste it wherever you wish.)\n\nRun the command below in the terminal to start Apache Airflow.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "start_airflow",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Please be patient, it may take upto 5 minutes for airflow to get started.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Exercise 2** - Open the Airflow Web UI\n\nCopy the Web-UI URL and paste it on a new browser tab. You can also click on the URL by holding the control key (Command key in case of a Mac).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You can Unpause/Pause a DAG using the Unpause/Pause toggle button.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Exercise 3** - List all DAGs\n\nApache airflow gives us some handy command line options to work with.\n\nRun the command below in the terminal to list out all the existing DAGs:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "airflow dags list",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Exercise 4** - List tasks in a DAG\nRun the command below in the terminal to list out all the tasks in the DAG named example_bash_operator.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "airflow tasks list example_bash_operator",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Also:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Run the command below in the terminal to unpause a DAG:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "airflow dags unpause tutorial",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Run the command below in the terminal to pause a DAG:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "airflow dags pause tutorial",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Create a DAG for Apache Airflow",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Objectives**\nAfter completing this lab you will be able to:\n\n- Explore the anatomy of a DAG.\n- Create a DAG.\n- Submit a DAG.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Exercise 3 - Explore the anatomy of a DAG\nAn Apache Airflow DAG is a python program. It consists of these logical blocks.\n\n- Imports\n- DAG Arguments\n- DAG Definition\n- Task Definitions\n- Task Pipeline\nA typical imports block looks like this.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**A typical imports block looks like this:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import the libraries\n\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow import DAG\n# Operators; we need this to write tasks!\nfrom airflow.operators.bash_operator import BashOperator\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**A typical DAG Arguments block looks like this:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#defining DAG arguments\n\n# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Ramesh Sannareddy',\n    'start_date': days_ago(0),\n    'email': ['ramesh@somemail.com'],\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "DAG arguments are like settings for the DAG.\n\nThe above settings mention:\n\n- the owner name,\n- when this DAG should run from: days_age(0) means today,\n- the email address where the alerts are sent to,\n- whether alert must be sent on failure,\n- whether alert must be sent on retry,\n- the number of retries in case of failure, and\n- the time delay between retries.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**A typical DAG definition block looks like this:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# define the DAG\ndag = DAG(\n    dag_id='sample-etl-dag',\n    default_args=default_args,\n    description='Sample ETL DAG using Bash',\n    schedule_interval=timedelta(days=1),\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Here we are creating a variable named dag by instantiating the DAG class with the following parameters.\n\n- sample-etl-dag is the ID of the DAG. This is what you see on the web console.\n\n- We are passing the dictionary default_args, in which all the defaults are defined.\n\n- description helps us in understanding what this DAG does.\n\n- schedule_interval tells us how frequently this DAG runs. In this case every day. (days=1).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**A typical task definitions block looks like this:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# define the tasks\n\n# define the first task named extract\nextract = BashOperator(\n    task_id='extract',\n    bash_command='echo \"extract\"',\n    dag=dag,\n)\n\n# define the second task named transform\ntransform = BashOperator(\n    task_id='transform',\n    bash_command='echo \"transform\"',\n    dag=dag,\n)\n\n# define the third task named load\n\nload = BashOperator(\n    task_id='load',\n    bash_command='echo \"load\"',\n    dag=dag,\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "A task is defined using:\n\n- A task_id which is a string and helps in identifying the task.\n- What bash command it represents.\n- Which dag this task belongs to.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**A typical task pipeline block looks like this:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# task pipeline\nextract >> transform >> load",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Task pipeline helps us to organize the order of tasks.\n\nHere the task **extract** must run first, followed by **transform**, followed by the task **load**.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Exercise 4 - Create a DAG",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let us create a DAG that runs daily, and extracts user information from datafile file, transforms it, and loads it into a file.\n\nThis DAG has two tasks extract that extracts fields from datafile file and transform_and_load that transforms and loads data into a file.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Create a new file \n- by choosing File->New File and \n- name it my_first_dag.py. \n- Copy the code above and paste it into my_first_dag.py",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import the libraries\n\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow import DAG\n# Operators; we need this to write tasks!\nfrom airflow.operators.bash_operator import BashOperator\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago\n\n#defining DAG arguments\n\n# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Ramesh Sannareddy',\n    'start_date': days_ago(0),\n    'email': ['ramesh@somemail.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# defining the DAG\n\n# define the DAG\ndag = DAG(\n    'my-first-dag',\n    default_args=default_args,\n    description='My first DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n# define the tasks\n\n# define the first task\n\nextract = BashOperator(\n    task_id='extract',\n    bash_command='cut -d\":\" -f1,3,6 datafile > /home/project/airflow/dags/extracted-data.txt',\n    dag=dag,\n)\n\n# define the second task\ntransform_and_load = BashOperator(\n    task_id='transform',\n    bash_command='tr \":\" \",\" < /home/project/airflow/dags/extracted-data.txt > /home/project/airflow/dags/transformed-data.csv',\n    dag=dag,\n)\n\n# task pipeline\nextract >> transform_and_load",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Exercise 5 - Submit a DAG**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Submitting a DAG is as simple as copying the DAG python file into dags folder in the AIRFLOW_HOME directory.\n\n- Open a terminal and run the command below to submit the DAG that was created in the previous exercise.\n\nNote: While submitting the dag that was created in the previous exercise, use sudo in the terminal before the command used to submit the dag.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cp my_first_dag.py $AIRFLOW_HOME/dags",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Verify that our DAG actually got submitted.\n\nRun the command below to list out all the existing DAGs.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Run the command below to list out all the existing DAGs.\n\nairflow dags list",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Verify that my-first-dag is a part of the output.\n\nairflow dags list|grep \"my-first-dag\"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Run the command below to list out all the tasks in my-first-dag\n\nairflow tasks list my-first-dag",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "airflow dags unpause tutorial",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Problem:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Write a DAG named ETL_Server_Access_Log_Processing.\n\n- Task 1: Create the imports block.\n\n- Task 2: Create the DAG Arguments block. You can use the default settings\n\n- Task 3: Create the DAG definition block. The DAG should run daily.\n\n- Task 4: Create the download task.\n\ndownload task must download the server access log file which is available at the URL: https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\n\n- Task 5: Create the extract task.\n\nThe server access log file contains these fields.\n\n    a. timestamp - TIMESTAMP\n\n    b. latitude - float\n\n    c. longitude - float\n\n    d. visitorid - char(37)\n\n    e. accessed_from_mobile - boolean\n\n    f. browser_code - int\n\nThe extract task must extract the fields timestamp and visitorid.\n\n- Task 6: Create the transform task.\n\nThe transform task must capitalize the visitorid.\n\n- Task 7: Create the load task.\n\nThe load task must compress the extracted and transformed data.\n\n- Task 8: Create the task pipeline block.\n\nThe pipeline block should schedule the task in the order listed below:\n\n-    download\n-    extract\n-    transform\n-    load\n\n- Task 10: Submit the DAG.\n\n- Task 11. Verify if the DAG is submitted",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Solution",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import the libraries\n\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow import DAG\n# Operators; we need this to write tasks!\nfrom airflow.operators.bash_operator import BashOperator\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago\n\n\n\n#defining DAG arguments\n\n# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Ramesh Sannareddy',\n    'start_date': days_ago(0),\n    'email': ['ramesh@somemail.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n\n\n\n# Defining the DAG\n\ndag = DAG(\n    'ETL_Server_Access_Log_Processing',\n    default_args=default_args,\n    description='My first DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n\n\n\n# define the tasks\n\n# Define the task 'download'\n\ndownload = BashOperator(\n    task_id='download',\n    bash_command='wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\"',\n    dag=dag,\n)\n\n\n\n# Define the task 'extract'\n\nextract = BashOperator(\n    task_id='extract',\n    bash_command='cut -f1,4 -d\"#\" web-server-access-log.txt > /home/project/airflow/dags/extracted.txt',\n    dag=dag,\n)\n\n\n\n# Define the task 'transform'\n\ntransform = BashOperator(\n    task_id='transform',\n    bash_command='tr \"[a-z]\" \"[A-Z]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.txt',\n    dag=dag,\n)\n\n\n\n\n# define the task 'load'\n\nload = BashOperator(\n    task_id='load',\n    bash_command='zip log.zip capitalized.txt' ,\n    dag=dag,\n)\n\n\n\n# task pipeline\n\ndownload >> extract >> transform >> load\n\n\n\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}