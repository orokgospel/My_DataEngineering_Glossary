{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Scenario**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I am a Data Engineer at an e-commerce company. I need to keep data synchronized between different databases/data warehouses as a part of my daily routine. \n\nOne task that I routinely performed is the sync up of staging data warehouse and production data warehouse. Automating this sync up do save me a lot of time and standardize my process. I will be using a given a set of python scripts to start with. I will use/modify them to perform the incremental data load from MySQL server which acts as a staging warehouse to the IBM DB2 or PostgreSQL which is a production data warehouse. This script will be scheduled by the data engineers to sync up the data between the staging and production data warehouse.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this Job task, I will write a python program that will:\n\n- Connect to IBM DB2 or PostgreSQL data warehouse and identify the last row on it.\n- Connect to MySQL staging data warehouse and find all rows later than the last row on the datawarehouse.\n- Insert the new data in the MySQL staging data warehouse into the IBM DB2 or PostgreSQL production data warehouse.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Software Required",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- MySQL Server\n- IBM DB2 or PostgreSQL",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Prepare my lab or computer environment",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Before you start the assignment:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 1:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Start MySQL server",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 2:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Create a database named sales",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mysql> CREATE DATABASE sales;\nQuery OK, 1 row affected (0.01 sec)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mysql> USE sales;\nDatabase changed",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 3:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the file below:\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/sales.sql",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 4:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Import the data in the file sales.sql into the sales database.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mysql> USE sales;\nDatabase changed\nmysql> source sales.sql;",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "output",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Query OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected (0.03 sec)\n\nQuery OK, 964 rows affected (0.03 sec)\nRecords: 964  Duplicates: 0  Warnings: 0\n\nQuery OK, 946 rows affected (0.02 sec)\nRecords: 946  Duplicates: 0  Warnings: 0\n\nQuery OK, 946 rows affected (0.03 sec)\nRecords: 946  Duplicates: 0  Warnings: 0\n\nQuery OK, 947 rows affected (0.03 sec)\nRecords: 947  Duplicates: 0  Warnings: 0\n\nQuery OK, 947 rows affected (0.04 sec)\nRecords: 947  Duplicates: 0  Warnings: 0\n\nQuery OK, 947 rows affected (0.03 sec)\nRecords: 947  Duplicates: 0  Warnings: 0\n\nQuery OK, 947 rows affected (0.03 sec)\nRecords: 947  Duplicates: 0  Warnings: 0\n\nQuery OK, 947 rows affected (0.02 sec)\nRecords: 947  Duplicates: 0  Warnings: 0\n\nQuery OK, 946 rows affected (0.03 sec)\nRecords: 946  Duplicates: 0  Warnings: 0\n\nQuery OK, 947 rows affected (0.03 sec)\nRecords: 947  Duplicates: 0  Warnings: 0\n\nQuery OK, 937 rows affected (0.02 sec)\nRecords: 937  Duplicates: 0  Warnings: 0\n\nQuery OK, 929 rows affected (0.02 sec)\nRecords: 929  Duplicates: 0  Warnings: 0\n\nQuery OK, 929 rows affected (0.10 sec)\nRecords: 929  Duplicates: 0  Warnings: 0\n\nQuery OK, 929 rows affected (0.01 sec)\nRecords: 929  Duplicates: 0  Warnings: 0\n\nQuery OK, 628 rows affected (0.01 sec)\nRecords: 628  Duplicates: 0  Warnings: 0\n\nQuery OK, 0 rows affected (0.38 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nQuery OK, 13836 rows affected (0.30 sec)\nRecords: 13836  Duplicates: 0  Warnings: 0\n\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nQuery OK, 0 rows affected, 1 warning (0.00 sec)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 5:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Verify that you can access your cloud instance of IBM DB2 server.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 6:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the mysqlconnect.py python programs from link below.\nCan use wget link(www.....)\n\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/mysqlconnect.py",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 7:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " mysqlconnect.py has the sample code to help understand how to connect to MySQL using Python.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 8:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Modify mysqlconnect.py suitably and make sure you are able to connect to the MySQL server instance on the Theia environment.\n\nNote: Before executing mysqlconnect.py note that you install the connector using the command: ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "python3 -m pip install mysql-connector-python==8.0.31",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This program requires the python module mysql-connector-python to be installed.\n# Install it using the below command\n# pip3 install mysql-connector-python\n\nimport mysql.connector\n\n# connect to database\nconnection = mysql.connector.connect(user='root', password='MTQ4NTAtb3Jva2dv',host='127.0.0.1',database='sales')\n\n# create cursor\n\ncursor = connection.cursor()\n\n# create table\n\nSQL = \"\"\"CREATE TABLE IF NOT EXISTS products(\n\nrowid int(11) NOT NULL AUTO_INCREMENT PRIMARY KEY,\nproduct varchar(255) NOT NULL,\ncategory varchar(255) NOT NULL\n\n)\"\"\"\n\ncursor.execute(SQL)\n\nprint(\"Table created\")\n\n# insert data\n\nSQL = \"\"\"INSERT INTO products(product,category)\n\t VALUES\n\t (\"Television\",\"Electronics\"),\n\t (\"Laptop\",\"Electronics\"),\n\t (\"Mobile\",\"Electronics\")\n\t \"\"\"\n\ncursor.execute(SQL)\nconnection.commit()\n\n\n# query data\n\nSQL = \"SELECT * FROM products\"\n\ncursor.execute(SQL)\n\nfor row in cursor.fetchall():\n\tprint(row)\n\n# close connection\nconnection.close()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ".",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In order to complete the tasks below, I have the option to complete them on either:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- DB2 database (Option A) or \n- on PostgreSQL (Option B).\n\nFor Option A, Wil follow tasks 9-12.\n\nOption B, will follow tasks 13-16.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Option A:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Using DB2 as the data warehouse:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 9:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the db2connect.py python program from the link below.\nCan use wget link on command line:\n\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/db2connect.py",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "db2connect.py has the sample code to help you understand how to connect to the cloud instance of IBM DB2 using Python.\n\nNote: Before executing db2connect.py note that you install the connector using the command:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": " python3 -m pip install ibm-db",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 10:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Modify db2connect.py suitably and make sure I'm able to connect to my cloud instance of IBM DB2 from the Theia environment.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 11:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the file below:\nCan use wget link....\n\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/sales.csv",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 12:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Load sales.csv into a table named sales_data on your cloud instance of IBM DB2 database.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ".",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Option B:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Using PostgreSQL as the data warehouse:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 13:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the postgresqlconnect.py python program from the link below.\n\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/postgresqlconnect.py",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "postgresqlconnect.py has the sample code to help you understand how to connect to the PostgreSql data warehouse using Python.\n\n**Note:** Before executing postgresqlconnect.py note that you install the connector using the command: ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "python3 -m pip install psycopg2",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 14:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Modify postgresqlconnect.py suitably and make sure you are able to connect to PostgreSql from the Theia environment.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This program requires the python module ibm-db to be installed.\n# Install it using the below command\n# python3 -m pip install psycopg2\n\nimport psycopg2\n\n# connectction details\n\ndsn_hostname = '127.0.0.1'\ndsn_user='postgres'        # e.g. \"abc12345\"\ndsn_pwd ='MTM5MzMtbGFrc2ht'      # e.g. \"7dBZ3wWt9XN6$o0J\"\ndsn_port =\"5432\"                # e.g. \"50000\" \ndsn_database =\"postgres\"           # i.e. \"BLUDB\"\n\n\n# create connection\n\nconn = psycopg2.connect(\n   database=dsn_database, \n   user=dsn_user,\n   password=dsn_pwd,\n   host=dsn_hostname, \n   port= dsn_port\n)\n\n#Crreate a cursor onject using cursor() method\n\ncursor = conn.cursor()\n\n# create table\nSQL = \"\"\"CREATE TABLE IF NOT EXISTS products(rowid INTEGER PRIMARY KEY NOT NULL,product varchar(255) NOT NULL,category varchar(255) NOT NULL)\"\"\"\n\n# Execute the SQL statement\ncursor.execute(SQL)\n\nprint(\"Table created\")\n\n# insert data\n\ncursor.execute(\"INSERT INTO  products(rowid,product,category) VALUES(1,'Television','Electronics')\");\n\ncursor.execute(\"INSERT INTO  products(rowid,product,category) VALUES(2,'Laptop','Electronics')\");\n\ncursor.execute(\"INSERT INTO products(rowid,product,category) VALUES(3,'Mobile','Electronics')\");\n\nconn.commit()\n\n# insert list of Records\n\nlist_ofrecords =[(5,'Mobile','Electronics'),(6,'Mobile','Electronics')]\n\ncursor = conn.cursor()\n\nfor row in list_ofrecords:\n  \n   SQL=\"INSERT INTO products(rowid,product,category) values(%s,%s,%s)\" \n   cursor.execute(SQL,row);\n   conn.commit()\n\n# query data\n\ncursor.execute('SELECT * from products;')\nrows = cursor.fetchall()\nconn.commit()\nconn.close()\nfor row in rows:\n    print(row)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Then run it, to Test;",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "python postgresqlconnect.py",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 15:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the file below: Can use wget on command line:\n\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/sales.csv",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 16:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Create a table called sales_data using the columns:\n- rowid, \n- product_id, \n- customer_id, \n- price, \n- quantity\n- timeestamp. \n\nLoad sales.csv into the table sales_data on your PostgreSql database.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "postgres>#\n\nCREATE TABLE sales_data (\n    rowid SERIAL PRIMARY KEY,\n    product_id INTEGER,\n    customer_id INTEGER,\n    price NUMERIC,\n    quantity INTEGER,\n    timestamp TIMESTAMP\n);",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Step 17:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Download the automation.py from the following URL:\nhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/automation.py\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Import libraries required for connecting to mysql\n\n# Import libraries required for connecting to DB2 or PostgreSql\n\n# Connect to MySQL\n\n# Connect to DB2 or PostgreSql\n\n# Find out the last rowid from DB2 data warehouse or PostgreSql data warehouse\n# The function get_last_rowid must return the last rowid of the table sales_data on the IBM DB2 database or PostgreSql.\n\ndef get_last_rowid():\n\tpass\n\n\nlast_row_id = get_last_rowid()\nprint(\"Last row id on production datawarehouse = \", last_row_id)\n\n# List out all records in MySQL database with rowid greater than the one on the Data warehouse\n# The function get_latest_records must return a list of all records that have a rowid greater than the last_row_id in the sales_data table in the sales database on the MySQL staging data warehouse.\n\ndef get_latest_records(rowid):\n\tpass\t\n\nnew_records = get_latest_records(last_row_id)\n\nprint(\"New rows on staging datawarehouse = \", len(new_records))\n\n# Insert the additional records from MySQL into DB2 or PostgreSql data warehouse.\n# The function insert_records must insert all the records passed to it into the sales_data table in IBM DB2 database or PostgreSql.\n\ndef insert_records(records):\n\tpass\n\ninsert_records(new_records)\nprint(\"New rows inserted into production datawarehouse = \", len(new_records))\n\n# disconnect from mysql warehouse\n\n# disconnect from DB2 or PostgreSql data warehouse \n\n# End of program",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You will be using automation.py as a scafolding program to execute the tasks in this assignment",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Exercise 1**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Automate loading of incremental data into the data warehouse",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "One of the routine tasks that is carried out around a data warehouse is the extraction of daily new data from the operational database and loading it into the data warehouse. In this exercise you will automate the extraction of incremental data, and loading it into the data warehouse.\n\nIn order to complete Tasks 1 and 3 below, you have an option to complete the tasks on a DB2 database (Option A), or on PostgreSQL (Option B).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Task 1** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Implement the function get_last_rowid()\n\nIn the program **automation.py** implement the function **get_last_rowid()**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Option A:**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "If you choose DB2 as the data warehouse:\n\nThis function must connect to the DB2 data warehouse and return the last rowid.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Option B:** ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "If you choose PostgreSQL as the data warehouse:\n    \nThis function must connect to the PostgreSql as the data warehouse and return the last rowid.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Find out the last rowid from DB2 data warehouse or PostgreSql data warehouse\n# The function get_last_rowid must return the last rowid of the table sales_data on the IBM DB2 database or PostgreSql.\n\ndef get_last_rowid():\n    \n    # Create a cursor for PostgreSQL\n    pg_cursor = conn.cursor()\n    \n    # Execute the SQL query to get the last rowid\n    pg_cursor.execute(\"SELECT MAX(rowid) FROM sales_data\")\n    \n    # Fetch the result\n    last_rowid = pg_cursor.fetchone()[0]\n    \n    # Close the cursor\n    pg_cursor.close()\n    \n    # Return the last rowid\n    return last_rowid\n\nlast_row_id = get_last_rowid()\nprint(\"Last row id on production datawarehouse = \", last_row_id)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Task 2**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Implement the function get_latest_records()\nIn the program automation.py implement the function get_latest_records()\n\nThis function must connect to the MySQL database and return all records later than the given last_rowid.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_latest_records(rowid):\n    \n    # Connect to the MySQL database\n    connection = mysql.connector.connect(\n        host='127.0.0.1',\n        user='root',\n        password='MTQ4NTAtb3Jva2dv',\n        database='sales'\n    )\n    cursor = connection.cursor()\n    query = \"SELECT * FROM sales_data WHERE rowid > {last_row_id}\"\n    cursor.execute(query)\n    records = cursor.fetchall()\n    return records\n\nnew_records = get_latest_records(last_row_id)\nprint(\"New rows on staging datawarehouse = \", len(new_records))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Task 3**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Implement the function insert_records()\n\nIn the program automation.py implement the function insert_records()\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##   Option A:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "If you choose DB2 as the data warehouse:\n\nThis function must connect to the DB2 data warehouse and insert all the given records.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Option B:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " If you choose PostgreSQL as the data warehouse:\n        \nThis function must connect to the PostgreSQL data warehouse and insert all the given records.\nTake a screenshot of the python code clearly showing the implementation of the function insert_records().",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def insert_records(records):\n    \n    # Create a cursor for PostgreSQL\n    pg_cursor = conn.cursor()\n\n    for record in records:\n        query = \"INSERT INTO sales_data (product_id, customer_id, price, quantity, timestamp) VALUES {record}\"\n        pg_cursor.execute(query)\n        \n    # Commit the changes and close the database connection\n    connection.commit()\n\ninsert_records(new_records)\nprint(\"New rows inserted into production datawarehouse = \", len(new_records))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Task 4**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " - Test the data synchronization\n \n\nRun the program automation.py and test if the synchronization is happening as expected.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "python automation.py",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Import libraries required for connecting to mysql\nimport mysql.connector\n\n# Import libraries required for connecting to DB2 or PostgreSql\nimport psycopg2\n\n# Connect to MySQL\n# connect to database\nconnection = mysql.connector.connect(user='root', password='MTQ4NTAtb3Jva2dv',host='127.0.0.1',database='sales')\n\n# create cursor\ncursor = connection.cursor()\n\n# Connect to DB2 or PostgreSql\nconn = psycopg2.connect(\n   database= postgres, \n   user=root,\n   password=MTA3NTQtb3Jva2dv,\n   host=localhost, \n   port= 5432\n)\n\n\n# Find out the last rowid from DB2 data warehouse or PostgreSql data warehouse\n# The function get_last_rowid must return the last rowid of the table sales_data on the IBM DB2 database or PostgreSql.\n\ndef get_last_rowid():\n    # Create a cursor for PostgreSQL\n    pg_cursor = conn.cursor()\n    # Execute the SQL query to get the last rowid\n    pg_cursor.execute(\"SELECT MAX(rowid) FROM sales_data\")\n    # Fetch the result\n    last_rowid = pg_cursor.fetchone()[0]\n    # Close the cursor\n    pg_cursor.close()\n    # Return the last rowid\n    return last_rowid\nlast_row_id = get_last_rowid()\nprint(\"Last row id on production datawarehouse = \", last_row_id)\n\n\n\n# List out all records in MySQL database with rowid greater than the one on the Data warehouse\n# The function get_latest_records must return a list of all records that have a rowid greater than the last_row_id in the sales_data table in the sales database on the MySQL staging data warehouse.\n\ndef get_latest_records(rowid):\n    # Connect to the MySQL database\n    connection = mysql.connector.connect(\n        host='127.0.0.1',\n        user='root',\n        password='MTQ4NTAtb3Jva2dv',\n        database='sales'\n    )\n    cursor = connection.cursor()\n    query = \"SELECT * FROM sales_data WHERE rowid > {last_row_id}\"\n    cursor.execute(query)\n    records = cursor.fetchall()\n    return records\t\nnew_records = get_latest_records(last_row_id)\nprint(\"New rows on staging datawarehouse = \", len(new_records))\n\n\n\n\n# Insert the additional records from MySQL into DB2 or PostgreSql data warehouse.\n# The function insert_records must insert all the records passed to it into the sales_data table in IBM DB2 database or PostgreSql.\n\ndef insert_records(records):\n    # Create a cursor for PostgreSQL\n    pg_cursor = conn.cursor()\n\n    for record in records:\n        query = \"INSERT INTO sales_data (product_id, customer_id, price, quantity, timestamp) VALUES {record}\"\n        pg_cursor.execute(query)\n    # Commit the changes and close the database connection\n    connection.commit()\n\ninsert_records(new_records)\nprint(\"New rows inserted into production datawarehouse = \", len(new_records))\n\n# disconnect from mysql warehouse\ncursor.close()\nconnection.close()\n\n# disconnect from DB2 or PostgreSql data warehouse \nconn.close()\n# End of program",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}