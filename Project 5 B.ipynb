{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Data Pipelines Using Apache AirFlow**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Scenario",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Write a pipeline that analyzes the web server log file, extracts the required lines(ending with html) and fields(time stamp, size ) and transforms (bytes to mb) and load (append to an existing file.)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Objectives**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this assignment you will author an Apache Airflow DAG that will:\n\n- Extract data from a web server log file\n- Transform the data\n- Load the transformed data into a tar file",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Tools / Software",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Apache AirFlow",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Exercise 1**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Prepare the lab environment\nBefore you start the assignment:\n\nStart Apache Airflow.\nDownload the dataset from the source to the destination mentioned below.\nSource : https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/accesslog.txt\n\nDestination : /home/project/airflow/dags/capstone",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "theia@theiadocker-orokgospel:/home/project$ cd airflow\ntheia@theiadocker-orokgospel:/home/project/airflow$ cd dags\ntheia@theiadocker-orokgospel:/home/project/airflow/dags$ mkdir capstone \ntheia@theiadocker-orokgospel:/home/project/airflow/dags$ wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/ETL/accesslog.txt",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## **Exercise 2**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Create a DAG",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Task 1**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Define the DAG arguments\n\nCreate a DAG with these arguments.\n\n- owner\n- start_date\n- email\n\nYou may define any suitable additional arguments.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Defining DAG arguments\n\n# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Gospel Orok',\n    'start_date': days_ago(0),\n    'email': ['orokgospel@gmeil.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Task 2**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Define the DAG\n\nCreate a DAG named process_web_log that runs daily.\n\nUsing suitable description.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Defining the DAG\n\ndag = DAG(\n    'process_web_log',\n    default_args=default_args,\n    description='My ETL Capstone DAG',\n    schedule_interval=timedelta(days=1),\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Task 3**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Create a task to extract data\n\nCreate a task named extract_data.\n\nThis task should extract the ipaddress field from the web server log file and save it into a file named extracted_data.txt",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Define the task 'extract'\nextract = BashOperator(\n    task_id='extract',\n    bash_command='cut -f1,4 -d\"#\" accesslog.txt > /home/project/airflow/dags/capstone/accesslog.txt',\n    dag=dag,\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# define the task 'transform'\ntransform = BashOperator(\n    task_id='transform',\n    bash_command='tr \"[a-z]\" \"[A-Z]\" < /home/project/airflow/dags/capstone/accesslog.txt't > /home/project/airflow/dags/capstone/accesslog.txt',\n    dag=dag,\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}