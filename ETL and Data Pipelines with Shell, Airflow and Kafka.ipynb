{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **ETL and Data Pipelines with Shell, Airflow and Kafka**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**ETL Fundamentals**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "ETL is an automated data pipeline engineering methodology.\n\n\nExtract:To extract data is to configure access to it and read it into an application. Normally this is an automated process. Some common methods include: \n- Web scraping, where data is extracted from web pages using applications such as Python or R to parse the underlying HTML code, and \n- Using APIs to programmatically connect to data and\n- query \n\nTransformation: This can include any of the following kinds of processes: \n- Cleaning: fixing errors or missing values. \n- Filtering: selecting only what is needed. \n- Joining disparate data sources: \n- merging related data.\nCleaning: fixing any errors or missing values  \n\nFiltering: selecting only what is needed  \n\nJoining: merging disparate data sources  \n\nNormalizing: converting data to common units  \n\nData Structuring: converting one data format to another, such as JSON, XML, or CSV to database tables \n\nFeature Engineering: creating KPIs for dashboards or machine learning   \n\nAnonymizing and Encrypting: ensuring privacy and security \n\nSorting: ordering the data to improve search performance \n\nAggregating: summarizing granular data \n\n\n\nloading: Generally this just means writing data to some new destination environment. \nTypical destinations include: \n- databases, \n- data warehouses, and \n- data marts. \nThe key goal of data loading is to make the data readily available for ingestion by analytics applications so that end users can gain value from it\n\n\nFor the Data Pipeline: Online transaction processing (OLTP) systems don’t save historical data. Accordingly, ETL processes capture the transaction history and prepare it for subsequent analysis in an online analytical processing (OLAP) system. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**ELT Basics**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "ELT is an emergent trend.\n    \nELT is similar to ETL in that similar stages are involved but the order in which they are performed is different.\n\nELT processes are used for cases where flexibility, speed, and scalability are important. Cloud-based analytics platforms are ideally suited for handling Big Data and ELT processes in a cost-efficient manner. ELT is an emerging trend mainly because cloud platform technologies are enabling it.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Data Extraction Techniques**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "SQL, NoSQL, web scraping, and APIs are important techniques for extracting data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Data Transformation Techniques**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Schema-on-write is the conventional approach used in ETL pipelines, where the data must be conformed to a defined schema prior to loading to a destination, such as a relational database. \n\nSchema-on-read relates to the modern ELT approach, where the schema is applied to the raw data after reading it from the raw data storage.\n\nWays of losing information in transformation processes include:\n- filtering, \n- aggregation, \n- using edge computing devices, and \n- lossy data compression.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Data Loading Techniques**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are many techniques for loading data, some of which are:\n- “Full loading”: You can load an initial history into a database, after which \n- “incremental loading” is applied to insert new data or to update already loaded data.\n\n\nSome data loading techniques are scheduled, on-demand, and incremental. Data can be loaded in batches, or it can be streamed continuously into its destination. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}